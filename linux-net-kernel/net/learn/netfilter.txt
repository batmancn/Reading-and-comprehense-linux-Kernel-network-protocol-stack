http://bbs.chinaunix.net/thread-3749229-1-2.html          [网络子系统] linux-2.6.35.6 xtables&iptables&hipac [复制链接]

http://bbs.chinaunix.net/thread-4082396-1-2.html          [网络子系统] linux-2.6.35.6 nf_conntrack [复制链接]

http://bbs.chinaunix.net/thread-1970484-1-1.html          Snort 入侵检测系统源码分析 [复制链接]

https://upload.wikimedia.org/wikipedia/commons/3/37/Netfilter-packet-flow.svg
netfilter整体结构

http://bbs.chinaunix.net/forum.php?mod=viewthread&tid=3749208&fromuid=20171559  netfilter讲解


nf_conn中的子连接(IP_CT_RELATED)是什么？

例如FTP，会使用两个TCP连接，一个用于控制信道、一个用于数据信道(https://wenku.baidu.com/view/f80e67b281c758f5f71f6736.html)。那么首先发来的是控制报文(IP_CT_NEW)，然后是数据报文(IP_CT_RELATED)，那么需要手动在IP_CT_NEW的连接上添加IP_CT_RELATED连接。

这样做的原因是一次FTP操作，需要相同的NAT一类的功能。所以IP_CT_NEW与相关的IP_CT_RELATED这两个连接执行相同的操作(例如NAT)。


netfilter框架?

nf_hooks: 全局结构体存储所有netfilter。ipv4, ipv6, arp, linux bridge,等都在这注册了。
hook点永远是: PRE_ROUTING LOCAL_IN FORWARD LOCAL_OUT POST_ROUTING
只是在邋LB中FORWARD被叫做BROUTE，参看bridge_learn.md。

/net/netfilter/*: netfilter核心功能代码，hook, nf_conntrack, netlink, xt_match, xt_target
/net/ipv4/*: ipv4协议栈代码
 - /net/ipv4/netfilter.c: 注册static const struct nf_afinfo nf_ip_afinfo
 - /net/ipv4/netfilter/*: ipv4 netfilter核心代码，nf_conntrack_ipv4, nf_nat, ipt_match, ipt_target, iptables, arptables。
/net/ipv6: 忽略。
/net/bridge/: LB代码
 - /net/bridge/br_netfilter.c: 注册static struct nf_hook_ops br_nf_ops[]
 - /net/bridge/netfilter/*: 核心代码，ebtable, eb_nat, eb_match, eb_target
 	-- eb_match: 分为arp(ebt_arp.c), ipv4(ebt_ip.c), ipv6, stp, vlan, 802.3等。

/include/linux: 这是内核进程与用户进程(用户进程陷入内核)共享的头文件.
 - /include/linux/netfilter.h: 用于/net/netfilter/*，主要包括x_tables, xt_match,  xt_target
 - /include/linux/netfilter_arp.h: 用于/net/netfilter_arp/*，主要包括arp_tables, arp_match,  arp_target
 - /include/linux/netfilter_ipv4.h: 用于/net/netfilter_ipv4/*，主要包括ip_tables, ipt_match, ipt_target
 - /include/linux/netfilter_ipv6.h: 用于/net/netfilter_ipv6/*，主要包括ip6_tables, ip6t_match, ip6t_target
 - /include/linux/netfilter_ipv4.h: 用于/net/netfilter_bridge/*，主要包括eb_tables, ebt_match, ebt_target, eb_nat
/include/net: 网络子系统的内核头文件
 - /include/netfilter/*.h: net/netfilter/*的头文件，主要是nf_conntrack与nat
 - /include/netfilter/ipv4/*.h: net/netfilter/ipv4/*的头文件，主要是nf_conntrack_ipv4
 - /include/netfilter/ipv6/*.h: net/netfilter/ipv6/*的头文件，主要是nf_conntrack_ipv6


xtables/iptables/hipac ?

这些tables是用户填入的，但是http://bbs.chinaunix.net/forum.php?mod=viewthread&tid=3749229&fromuid=20171559
中说"由于写者（添加、删除）和读者（查找）都是在内核空间进程上下文执行，所以它们只需要用xt[n].mutex信号量进行互斥"
后面分析加表项的机制。

struct  xt_af  xt[]，这个用于存储match、target，操作他的有:
写者: 即iptables_ops中的添加、删除，这是上层用户通过netlink访问。
读者: 即iptables_ops中的查询，这是上层用户通过netlink访问。
内核软中断读者: 内核收包软中断下半部会运行TCPIP协议栈于此。

基于此，tables设计了两种同步方式:
xt[n].mutex保护table的读、写操作，引用计数方式用于table的资源释放。

net.xt.tables[]: struct net net; struct netns_xt xt; struct list_head tables[NFPROTO_NUMPROTO];
用于xt_register_table/xt_hook_link, xt_hook_unlink/xt_unregister_table。
用于注册、卸载ip_table, ip6_table, arp_table, eb_table等。
结构组成参见http://bbs.chinaunix.net/forum.php?mod=viewthread&tid=3749229&fromuid=20171559的2楼。
可以看出来net.xt.tables[]是struct xt_table。
操作net.xt.tables[]：ipt_register_table，主要功能是填充net.xt.table[IPV4]与xt_hook_link()函数注册一个HOOK点。


为什么区分不同的CPU？

因为不同CPU都会收到数据包。


锁机制如何设计的？

参考上面的URL。
xt_info_locks[CPU]
xt[af].mutex
xt[table->af].mutex