http://bbs.chinaunix.net/thread-3749229-1-2.html          [网络子系统] linux-2.6.35.6 xtables&iptables&hipac [复制链接]

http://bbs.chinaunix.net/thread-4082396-1-2.html          [网络子系统] linux-2.6.35.6 nf_conntrack [复制链接]

http://bbs.chinaunix.net/thread-1970484-1-1.html          Snort 入侵检测系统源码分析 [复制链接]

https://upload.wikimedia.org/wikipedia/commons/3/37/Netfilter-packet-flow.svg
netfilter整体结构

http://bbs.chinaunix.net/forum.php?mod=viewthread&tid=3749208&fromuid=20171559  netfilter讲解


nf_conn中的子连接(IP_CT_RELATED)是什么？
--------------------------------------------------------------

例如FTP，会使用两个TCP连接，一个用于控制信道、一个用于数据信道(https://wenku.baidu.com/view/f80e67b281c758f5f71f6736.html)。那么首先发来的是控制报文(IP_CT_NEW)，然后是数据报文(IP_CT_RELATED)，那么需要手动在IP_CT_NEW的连接上添加IP_CT_RELATED连接。

这样做的原因是一次FTP操作，需要相同的NAT一类的功能。所以IP_CT_NEW与相关的IP_CT_RELATED这两个连接执行相同的操作(例如NAT)。


netfilter框架?
-----------------

nf_hooks: 全局结构体存储所有netfilter。ipv4, ipv6, arp, linux bridge,等都在这注册了。
hook点永远是: PRE_ROUTING LOCAL_IN FORWARD LOCAL_OUT POST_ROUTING
只是在邋LB中FORWARD被叫做BROUTE，参看bridge_learn.md。

/net/netfilter/*: netfilter核心功能代码，hook, nf_conntrack, netlink, xt_match, xt_target
/net/ipv4/*: ipv4协议栈代码
 - /net/ipv4/netfilter.c: 注册static const struct nf_afinfo nf_ip_afinfo
 - /net/ipv4/netfilter/*: ipv4 netfilter核心代码，nf_conntrack_ipv4, nf_nat, ipt_match, ipt_target, iptables, arptables。
/net/ipv6: 忽略。
/net/bridge/: LB代码
 - /net/bridge/br_netfilter.c: 注册static struct nf_hook_ops br_nf_ops[]
 - /net/bridge/netfilter/*: 核心代码，ebtable, eb_nat, eb_match, eb_target
 	-- eb_match: 分为arp(ebt_arp.c), ipv4(ebt_ip.c), ipv6, stp, vlan, 802.3等。

/include/linux: 这是内核进程与用户进程(用户进程陷入内核)共享的头文件.
 - /include/linux/netfilter.h: 用于/net/netfilter/*，主要包括x_tables, xt_match,  xt_target
 - /include/linux/netfilter_arp.h: 用于/net/netfilter_arp/*，主要包括arp_tables, arp_match,  arp_target
 - /include/linux/netfilter_ipv4.h: 用于/net/netfilter_ipv4/*，主要包括ip_tables, ipt_match, ipt_target
 - /include/linux/netfilter_ipv6.h: 用于/net/netfilter_ipv6/*，主要包括ip6_tables, ip6t_match, ip6t_target
 - /include/linux/netfilter_ipv4.h: 用于/net/netfilter_bridge/*，主要包括eb_tables, ebt_match, ebt_target, eb_nat
/include/net: 网络子系统的内核头文件
 - /include/netfilter/*.h: net/netfilter/*的头文件，主要是nf_conntrack与nat
 - /include/netfilter/ipv4/*.h: net/netfilter/ipv4/*的头文件，主要是nf_conntrack_ipv4
 - /include/netfilter/ipv6/*.h: net/netfilter/ipv6/*的头文件，主要是nf_conntrack_ipv6


xtables/iptables/hipac ?
-------------------------

这些tables是用户填入的，但是http://bbs.chinaunix.net/forum.php?mod=viewthread&tid=3749229&fromuid=20171559
中说"由于写者（添加、删除）和读者（查找）都是在内核空间进程上下文执行，所以它们只需要用xt[n].mutex信号量进行互斥"
后面分析加表项的机制。

struct  xt_af  xt[]，这个用于存储match、target，操作他的有:
写者: 即iptables_ops中的添加、删除，这是上层用户通过netlink访问。
读者: 即iptables_ops中的查询，这是上层用户通过netlink访问。
内核软中断读者: 内核收包软中断下半部会运行TCPIP协议栈于此。

基于此，tables设计了两种同步方式:
xt[n].mutex保护table的读、写操作，引用计数方式用于table的资源释放。

net.xt.tables[]: struct net net; struct netns_xt xt; struct list_head tables[NFPROTO_NUMPROTO];
用于xt_register_table/xt_hook_link, xt_hook_unlink/xt_unregister_table。
用于注册、卸载ip_table, ip6_table, arp_table, eb_table等。
结构组成参见http://bbs.chinaunix.net/forum.php?mod=viewthread&tid=3749229&fromuid=20171559的2楼。
可以看出来net.xt.tables[]是struct xt_table。
操作net.xt.tables[]：ipt_register_table，主要功能是填充net.xt.table[IPV4]与xt_hook_link()函数注册一个HOOK点。


为什么区分不同的CPU？
-------------------------

因为不同CPU都会收到数据包。


锁机制如何设计的？
-------------------------

参考上面的URL。
xt_info_locks[CPU]
xt[af].mutex
xt[table->af].mutex


iptables包含的target?
-------------------------

Iptables包含以下target
IPT_ACCEPT：(struct ipt_standard_target *)target->verdict = -NF_ACCEPT-
1        < 0        （ipt_do_table()碰到这个target直接返回NF_ACCEPT）

IPT_DROP：(struct ipt_standard_target *)target->verdict = -NF_DROP-1 <
0                （ipt_do_table()碰到这个target直接返回NF_DROP）

IPT_QUEUE：(struct ipt_standard_target *)target->verdict = -NF_QUEUE-1 <
0        （ipt_do_table()碰到这个target直接返回NF_QUEUE）

IPT_RETURN：(struct ipt_standard_target *)target->verdict = -NF_REPATE-
1        < 0        （ipt_do_table()碰到这个target特殊处理）

跳转到子链target：(struct ipt_standard_target *)target->verdict =
要跳转子链的偏移量 > 0        （ipt_do_table()碰到这个target
会跳转到该子链处理）

IPT_CONTINUE：        IPT_CONTINUE = XT_CONTINUE = 0xFFFFFFFF < 0        （
ipt_do_table()碰到这个target会处理下一条规则，这个target被扩展target使用）(
它不是一个标准target，但可被其它扩展TARGET用作返回值)

扩展target：它是struct xt_entry_target + date[
]                                （ipt_do_table()碰到这个target会调用target->
target()处理，并根据返回值做处理。扩展target可返回IPT_CONTINUE，或下面
netfilter定义的值）

Netfilter处理的返回值
NF_DROP
NF_ACCEPT
NF_STOLEN
NF_QUEUE
NF_REPEAT
NF_STOP


nf_conn中的子连接(IP_CT_RELATED)是什么？
-------------------------

nf_conntrack.ko是这个模块。

例如FTP，会使用两个TCP连接，一个用于控制信道、一个用于数据信道(https://wenku.baidu.com/view/f80e67b281c758f5f71f6736.html)。那么首先发来的是控制报文(IP_CT_NEW)，然后是数据报文(IP_CT_RELATED)，那么需要手动在IP_CT_NEW的连接上添加IP_CT_RELATED连接。

这样做的原因是一次FTP操作，需要相同的NAT一类的功能。所以IP_CT_NEW与相关的IP_CT_RELATED这两个连接执行相同的操作(例如NAT)。


nf_conn重要结构?

连接跟踪，顾名思义，就是识别一个连接上双方向的数据包，
同时记录状态。下面看一下它的数据结构：

struct nf_conn {
        /* Usage count in here is 1 for hash table/destruct timer, 1 per skb, plus 1 for any connection(s) we are `master' for */

        struct  nf_conntrack  ct_general;                /* 连接跟踪的引用计数 */

        spinlock_t  lock;

        /* Connection tracking(链接跟踪)用来跟踪、记录每个链接的信息(目前仅支持IP协议的连接跟踪)。
            每个链接由“tuple”来唯一标识，这里的“tuple”对不同的协议会有不同的含义，例如对tcp,udp
                 来说就是五元组: (源IP，源端口，目的IP, 目的端口，协议号)，对ICMP协议来说是: (源IP, 目
            的IP, id, type, code), 其中id,type与code都是icmp协议的信息。链接跟踪是防火墙实现状态检
            测的基础，很多功能都需要借助链接跟踪才能实现，例如NAT、快速转发、等等。*/

        struct  nf_conntrack_tuple_hash  tuplehash[IP_CT_DIR_MAX];

        unsigned long  status;                             /* 可以设置由enum ip_conntrack_status中描述的状态 */


        struct  nf_conn  *master;                        /* 如果该连接是某个连接的子连接，则master指向它的主连接 */

        /* Timer function; drops refcnt when it goes off. */
        struct  timer_list  timeout;

        union nf_conntrack_proto proto;                /* 用于保存不同协议的私有数据 */

        /* Extensions */
        struct nf_ct_ext *ext;                        /* 用于扩展结构 */
};
这个结构非常简单，其中最主要的就是tuplehash（跟踪连接双方向数据）
和status（记录连接状态），这也连接跟踪最主要的功能。

在status中可以设置的标志，由下面的enum ip_conntrack_status描述，
它们可以共存。这些标志设置后就不会再被清除。
enum ip_conntrack_status {
        IPS_EXPECTED_BIT = 0,                /* 表示该连接是个子连接 */
        IPS_SEEN_REPLY_BIT = 1,                /*表示该连接上双方向上都有数据包了 */

        IPS_ASSURED_BIT = 2,                /* TCP：在三次握手建立完连接后即设定该标志。
        	UDP：如果在该连接上的两个方向都有数据包通过，
        	则再有数据包在该连接上通过时，就设定该标志。
        	ICMP：不设置该标志 */

        IPS_CONFIRMED_BIT = 3,                /* 表示该连接已被添加到net->ct.hash表中 */

        IPS_SRC_NAT_BIT = 4,                /*在POSTROUTING处，当替换replytuple完成时, 设置该标记 */

        IPS_DST_NAT_BIT = 5,                /* 在PREROUTING处，当替换replytuple完成时, 设置该标记 */

        /* Both together. */
        IPS_NAT_MASK = (IPS_DST_NAT | IPS_SRC_NAT),
        /* Connection needs TCP sequence adjusted. */
        IPS_SEQ_ADJUST_BIT = 6,
        IPS_SRC_NAT_DONE_BIT = 7,        /* 在POSTROUTING处，已被SNAT处理，并被加入到bysource链中，设置该标记 */

        IPS_DST_NAT_DONE_BIT = 8,        /* 在PREROUTING处，已被DNAT处理，并被加入到bysource链中，设置该标记 */

        /* Both together */
        IPS_NAT_DONE_MASK = (IPS_DST_NAT_DONE | IPS_SRC_NAT_DONE),
        IPS_DYING_BIT = 9,                /*表示该连接正在被释放，
        	内核通过该标志保证正在被释放的ct不会被其它地方再次引用。
        	有了这个标志，当某个连接要被删除时，即使它还在net->ct.hash中，
        	也不会再次被引用。*/

        IPS_FIXED_TIMEOUT_BIT = 10,        /*固定连接超时时间，
        	这将不根据状态修改连接超时时间。
        	通过函数nf_ct_refresh_acct()修改超时时间时检查该标志。 */

        IPS_TEMPLATE_BIT = 11,                /* 由CT target进行设置（这个target只能用在raw表中，
        	用于为数据包构建指定ct，并打上该标志），
        	用于表明这个ct是由CT target创建的 */

};

连接跟踪对该连接上的每个数据包表现为以下几种状态之一，由enum ip_conntrack_info
表示，被设置在skb->nfctinfo中。
enum ip_conntrack_info {
IP_CT_ESTABLISHED（0）,         /*表示这个数据包对应的连接在两个方向
	都有数据包通过，并且这是ORIGINAL初始方向数据包
	（无论是TCP、UDP、ICMP数据包，只要在该连接的两个方向
	上已有数据包通过，就会将该连接设置为IP_CT_ESTABLISHED状态。
	不会根据协议中的标志位进行判断，例如TCP的SYN等）。
	但它表示不了这是第几个数据包，也说明不了这个CT是否是子连接。*/

IP_CT_RELATED（1）,                /* 表示这个数据包对应的连接还没有REPLY
	方向数据包，当前数据包是ORIGINAL方向数据包。
	并且这个连接关联一个已有的连接，是该已有连接的子连接，
	（即status标志中已经设置了IPS_EXPECTED标志，该标志在
	init_conntrack()函数中设置）。但无法判断是第几个数据包
	（不一定是第一个）*/

IP_CT_NEW（2）,                        /* 表示这个数据包对应的连接还没有REPLY
	方向数据包，当前数据包是ORIGINAL方向数据包，该连接不是子连接。
	但无法判断是第几个数据包（不一定是第一个）*/

IP_CT_IS_REPLY（3）,                /*这个状态一般不单独使用，
	通常以下面两种方式使用 */

IP_CT_ESTABLISHED + IP_CT_IS_REPLY（3）,        /*表示这个数据包对应的连接在两个
	方向都有数据包通过，并且这是REPLY应答方向数据包。
	但它表示不了这是第几个数据包，也说明不了这个CT是否是子连接。*/

IP_CT_RELATED + IP_CT_IS_REPLY（4）,                /* 这个状态仅在
	nf_conntrack_attach()函数中设置，用于本机返回REJECT，例如返回一个ICMP
	目的不可达报文，或返回一个reset报文。
	它表示不了这是第几个数据包。*/

IP_CT_NUMBER = IP_CT_IS_REPLY * 2 - 1（5）        /* 可表示状态的总数 */
};

以上就是连接跟踪里最重要的数据结构了，
用于跟踪连接、记录状态、并对该连接的每个数据包设置一种状态。


未创建子连接时候nf_conn结构组织?
-------------------------

参考链接2楼。非常重要。关键点:
struct netns_ct {
	...,
	*stat -> ip_conntrack_stat
	*hash -> hlist_nulls_head(链接hash表，所有链接都放在这里)
		-> struct nf_conn(一个连接，参考上面的解释)
	*expect_hash -> hlist_head(expect连接表)
		-> struct 	nf_conntrack_expect(一个expect，表示主链接期待的连接，由helper函数创建，当找到该期待的连接后这个结构释放)
			-> struct hlist_head_nf_helper_hash[]
				-> nf_conntrack_helper(这个就是期待的连接)
	...,
}


创建子连接的结构?
-------------------------

参考3楼。


netns_ct?
----------

struct netns_ct {
        atomic_t                        count;                                /* 当前连接表中连接的个数 */

        unsigned int                expect_count;                        /* nf_conntrack_helper创建的期待子连接
        											nf_conntrack_expect项的个数 */

        unsigned int                htable_size;                        /*存储连接（nf_conn）的HASH桶的大小 */

        struct kmem_cache        *nf_conntrack_cachep;                /*指向用于分配nf_conn结构而建立的高速缓存（slab）对象 */

        struct hlist_nulls_head        *hash;	/* 指向存储连接（nf_conn）的HASH桶 */

        struct hlist_head                *expect_hash;/* 指向存储期待子连接nf_conntrack_expect项的HASH桶 */

        struct hlist_nulls_head        unconfirmed;                        /*对于一个链接的第一个包，
        	在init_conntrack()函数中会将该包original方向的tuple结构挂入该链，这
		是因为在此时还不确定该链接会不会被后续的规则过滤掉，
		如果被过滤掉就没有必要挂入正式的链接跟踪表。

		在ipv4_confirm()函数中，会将unconfirmed链中的tuple拆掉，然后再将
		original方向和reply方向的tuple挂入到正式的链接跟踪表中，
		即init_net.ct.hash中，这是因为到达ipv4_confirm()函数时，
		应经在钩子NF_IP_POST_ROUTING处了，已经通过了前面的filter表。
		通过cat  /proc/net/nf_conntrack显示连接，是不会显示该链中的连接的。
		但总的连接个数（net->ct.count）包含该链中的连接。

		当注销l3proto、l4proto、helper、nat等资源或在应用层删除所有连接
		（conntrack -F）时，除了释放confirmed连接（在net->ct.hash中的连接）
		的资源，还要释放unconfirmed连接（即在该链中的连接）的资源。*/

        struct hlist_nulls_head        dying;	/* 释放连接时，通告DESTROY事件失败的ct
		被放入该链中，并设置定时器，等待下次通告。
		通过cat  /proc/net/nf_conntrack显示连接，是不会显示该链中的连接的。
		但总的连接个数（net->ct.count）包含该链中的连接。

		当注销连接跟踪模块时，同时要清除正再等待被释放的连接
		（即该链中的连接）*/

        struct ip_conntrack_stat        __percpu *stat;    /* 连接跟踪过程中的一些状态统计，
        	每个CPU一项，目的是为了减少锁 */

        int                        sysctl_events;                        /*是否开启连接事件通告功能 */

        unsigned int                sysctl_events_retry_timeout;        /*通告失败后，
        	重试通告的间隔时间，单位是秒 */

        int                        sysctl_acct;                        /*是否开启每个连接数据包统计功能 */

        int                        sysctl_checksum;
        unsigned int                sysctl_log_invalid;                 /*Log invalid packets */

#ifdef CONFIG_SYSCTL
        struct ctl_table_header        *sysctl_header;
        struct ctl_table_header        *acct_sysctl_header;
        struct ctl_table_header        *event_sysctl_header;
#endif
        int                        hash_vmalloc;                        /*存储连接（nf_conn）的HASH桶
        	是否是使用vmalloc()进行分配的 */

        int                        expect_vmalloc;                        /*存储期待子连接nf_conntrack_expect
        	项的HASH桶是否是使用vmalloc()进行分配的 */

        char                        *slabname;                        /*用于分配nf_conn结构而建立的
        	高速缓存（slab）对象的名字 */

};


ipv4的nf_conn模块?
-------------------------

不同协议有不同的nf_conn模块，目前内置ipv4。其他的要自己开发。
